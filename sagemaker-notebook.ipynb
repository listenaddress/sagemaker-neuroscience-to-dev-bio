{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Getting Started Demo\n",
    "### Binary Classification with `Trainer` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using the `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.48.0 in /opt/conda/lib/python3.6/site-packages (2.93.0)\n",
      "Requirement already satisfied: transformers==4.12.3 in /opt/conda/lib/python3.6/site-packages (4.12.3)\n",
      "Requirement already satisfied: datasets[s3]==1.18.3 in /opt/conda/lib/python3.6/site-packages (1.18.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (1.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (0.4.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (3.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (2.25.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (4.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (2022.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (4.64.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (0.0.53)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from transformers==4.12.3) (0.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (3.0.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (0.3.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (1.1.5)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (3.8.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (2022.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (0.70.11.1)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (6.0.1)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (0.4.2)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (1.23.10)\n",
      "Requirement already satisfied: botocore in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.18.3) (1.26.10)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.15.8)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.7)\n",
      "Requirement already satisfied: attrs==20.3.0 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (20.3.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.6/site-packages (from boto3->datasets[s3]==1.18.3) (0.5.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->datasets[s3]==1.18.3) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore->datasets[s3]==1.18.3) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore->datasets[s3]==1.18.3) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.12.3) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->transformers==4.12.3) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers==4.12.3) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf<4.0,>=3.1->sagemaker>=2.48.0) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.12.3) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.12.3) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers==4.12.3) (2.10)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.6/site-packages (from tqdm>=4.27->transformers==4.12.3) (5.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (5.2.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (4.0.2)\n",
      "Requirement already satisfied: idna-ssl>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.6/site-packages (from aiohttp->datasets[s3]==1.18.3) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets[s3]==1.18.3) (2021.1)\n",
      "Requirement already satisfied: pox>=0.2.9 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /opt/conda/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.12.3) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.12.3) (7.1.2)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch\n",
    "# !pip install -q datasets\n",
    "!pip install -q sentencepiece\n",
    "# !pip install -q torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::641078039118:role/service-role/AmazonSageMaker-ExecutionRole-20220603T174026\n",
      "sagemaker bucket: sagemaker-us-east-1-641078039118\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `imdb` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [imdb](http://ai.stanford.edu/~amaas/data/sentiment/) dataset consists of 25000 training and 25000 testing highly polar movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # tokenizer used in preprocessing\n",
    "# tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# # dataset used\n",
    "# dataset_name = 'imdb'\n",
    "\n",
    "# # s3 key prefix for the data\n",
    "# s3_prefix = 'samples/datasets/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# dataset = load_dataset(dataset_name)\n",
    "\n",
    "# # download tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# # tokenizer helper function\n",
    "# def tokenize(batch):\n",
    "#     return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# # load dataset\n",
    "# train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "# test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# # tokenize dataset\n",
    "# train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "# test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# # set format for pytorch\n",
    "# train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "# test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import botocore\n",
    "# from datasets.filesystems import S3FileSystem\n",
    "\n",
    "# s3 = S3FileSystem()  \n",
    "\n",
    "# # save train_dataset to s3\n",
    "# training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "# train_dataset.save_to_disk(training_input_path,fs=s3)\n",
    "\n",
    "# # save test_dataset to s3\n",
    "# test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "# test_dataset.save_to_disk(test_input_path,fs=s3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator’s fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,\n",
    "                 'model_name':'neuroscience-to-dev-bio'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='neuroscience_to_dev_bio.py',\n",
    "                            source_dir='./scripts',\n",
    "                            instance_type='ml.g4dn.16xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            transformers_version='4.12',\n",
    "                            pytorch_version='1.9',\n",
    "                            py_version='py38',\n",
    "                            hyperparameters = hyperparameters,\n",
    "                            volume_size=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.volume_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t    .sagemaker-jumpstart-tasks-status.json\n",
      "..\t\t    .yarnrc\n",
      ".aws\t\t    Untitled.ipynb\n",
      ".cache\t\t    dld.csv\n",
      ".canvas\t\t    dld.txt\n",
      ".config\t\t    neuroscience-to-dev-bio.ipynb\n",
      ".ipynb_checkpoints  sagemaker-notebook.ipynb\n",
      ".ipython\t    scripts\n",
      ".jupyter\t    test.txt\n",
      ".local\t\t    train.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 12:06:39 Starting - Starting the training job...\n",
      "2022-06-05 12:06:55 Starting - Preparing the instances for trainingProfilerReport-1654430798: InProgress\n",
      ".........\n",
      "2022-06-05 12:08:26 Downloading - Downloading input data\n",
      "2022-06-05 12:08:26 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-05 12:12:50,216 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-05 12:12:50,235 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-05 12:12:50,241 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-05 12:12:50,654 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.1.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (2021.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 3)) (1.19.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nltk, absl-py, rouge-score\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.1.0 nltk-3.7 rouge-score-0.0.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-06-05 12:12:53,633 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"neuroscience-to-dev-bio\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-06-05-12-06-38-531\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-12-06-38-531/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"neuroscience_to_dev_bio\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"neuroscience_to_dev_bio.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"neuroscience-to-dev-bio\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=neuroscience_to_dev_bio.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=neuroscience_to_dev_bio\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-12-06-38-531/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"neuroscience-to-dev-bio\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-06-05-12-06-38-531\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-12-06-38-531/source/sourcedir.tar.gz\",\"module_name\":\"neuroscience_to_dev_bio\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"neuroscience_to_dev_bio.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"neuroscience-to-dev-bio\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=neuroscience-to-dev-bio\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience-to-dev-bio\u001b[0m\n",
      "\n",
      "2022-06-05 12:13:08 Training - Training image download completed. Training in progress.\u001b[34m**** TRAINING!\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 5.61kB [00:00, 5.13MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/3.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 3.02k/3.02k [00:00<00:00, 3.90MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 3.45M/2.12G [00:00<01:02, 36.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 7.65M/2.12G [00:00<00:55, 40.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 17.4M/2.12G [00:00<00:32, 68.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 26.2M/2.12G [00:00<00:28, 78.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 33.6M/2.12G [00:00<00:42, 52.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 40.0M/2.12G [00:00<00:45, 49.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 49.5M/2.12G [00:00<00:35, 61.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 56.2M/2.12G [00:01<00:38, 57.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 64.0M/2.12G [00:01<00:39, 55.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 72.1M/2.12G [00:01<00:36, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▎         | 79.5M/2.12G [00:01<00:33, 65.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 86.1M/2.12G [00:01<00:37, 58.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 92.0M/2.12G [00:01<00:43, 50.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 97.1M/2.12G [00:01<00:51, 42.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 104M/2.12G [00:02<00:46, 46.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 112M/2.12G [00:02<00:51, 42.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 118M/2.12G [00:02<00:54, 39.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 122M/2.12G [00:02<00:53, 40.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 128M/2.12G [00:02<00:53, 39.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 134M/2.12G [00:02<00:52, 40.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 138M/2.12G [00:03<00:56, 38.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 145M/2.12G [00:03<00:47, 44.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 152M/2.12G [00:03<00:42, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 161M/2.12G [00:03<00:34, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 168M/2.12G [00:03<00:38, 55.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 176M/2.12G [00:03<00:34, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 184M/2.12G [00:03<00:31, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 190M/2.12G [00:03<00:32, 63.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 196M/2.12G [00:03<00:38, 53.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 202M/2.12G [00:04<00:45, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 208M/2.12G [00:04<00:41, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 216M/2.12G [00:04<00:35, 57.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 224M/2.12G [00:04<00:35, 57.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 234M/2.12G [00:04<00:29, 67.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 240M/2.12G [00:04<00:32, 62.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█▏        | 248M/2.12G [00:04<00:36, 54.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 256M/2.12G [00:05<00:33, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 264M/2.12G [00:05<00:35, 56.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 271M/2.12G [00:05<00:32, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 278M/2.12G [00:05<00:37, 53.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 286M/2.12G [00:05<00:31, 62.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 293M/2.12G [00:05<00:32, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 299M/2.12G [00:05<00:37, 52.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 306M/2.12G [00:05<00:33, 58.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 316M/2.12G [00:06<00:28, 68.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 323M/2.12G [00:06<00:31, 60.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 329M/2.12G [00:06<00:31, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 336M/2.12G [00:06<00:30, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 344M/2.12G [00:06<00:28, 66.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 351M/2.12G [00:06<00:27, 68.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 359M/2.12G [00:06<00:26, 72.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 368M/2.12G [00:06<00:23, 79.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 376M/2.12G [00:06<00:25, 74.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 384M/2.12G [00:07<00:24, 75.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 392M/2.12G [00:07<00:29, 63.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 400M/2.12G [00:07<00:29, 63.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 408M/2.12G [00:07<00:29, 63.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 416M/2.12G [00:07<00:28, 64.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 424M/2.12G [00:07<00:29, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 430M/2.12G [00:07<00:29, 62.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 436M/2.12G [00:08<00:29, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 442M/2.12G [00:08<00:41, 43.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 448M/2.12G [00:08<00:39, 46.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 456M/2.12G [00:08<00:35, 50.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██▏       | 464M/2.12G [00:08<00:33, 53.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 470M/2.12G [00:08<00:32, 55.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 476M/2.12G [00:08<00:34, 52.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 484M/2.12G [00:09<00:29, 60.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 490M/2.12G [00:09<00:34, 51.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 496M/2.12G [00:09<00:32, 54.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 502M/2.12G [00:09<00:31, 55.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 508M/2.12G [00:09<00:32, 53.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 513M/2.12G [00:09<00:36, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 520M/2.12G [00:09<00:36, 47.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 529M/2.12G [00:09<00:28, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 536M/2.12G [00:10<00:29, 57.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 544M/2.12G [00:10<00:26, 64.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 552M/2.12G [00:10<00:30, 56.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 558M/2.12G [00:10<00:32, 52.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 564M/2.12G [00:10<00:33, 50.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 569M/2.12G [00:10<00:32, 51.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▋       | 575M/2.12G [00:10<00:34, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 579M/2.12G [00:10<00:34, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 584M/2.12G [00:11<00:47, 34.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 591M/2.12G [00:11<00:44, 37.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 595M/2.12G [00:11<00:44, 37.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 600M/2.12G [00:11<00:39, 41.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 608M/2.12G [00:11<00:36, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 614M/2.12G [00:11<00:35, 46.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▊       | 620M/2.12G [00:11<00:33, 48.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 625M/2.12G [00:12<00:33, 48.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 632M/2.12G [00:12<00:31, 52.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 640M/2.12G [00:12<00:26, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 648M/2.12G [00:12<00:27, 57.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 656M/2.12G [00:12<00:27, 58.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 662M/2.12G [00:12<00:28, 56.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 668M/2.12G [00:12<00:30, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 673M/2.12G [00:12<00:30, 50.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 678M/2.12G [00:13<00:31, 50.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███▏      | 682M/2.12G [00:13<00:39, 39.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 688M/2.12G [00:13<00:39, 39.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 696M/2.12G [00:13<00:31, 49.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 701M/2.12G [00:13<00:35, 43.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 706M/2.12G [00:13<00:36, 42.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 716M/2.12G [00:13<00:26, 57.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 722M/2.12G [00:14<00:24, 60.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 729M/2.12G [00:14<00:23, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 736M/2.12G [00:14<00:23, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 744M/2.12G [00:14<00:26, 56.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 750M/2.12G [00:14<00:28, 52.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 756M/2.12G [00:14<00:30, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 762M/2.12G [00:14<00:27, 54.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 769M/2.12G [00:14<00:25, 58.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 776M/2.12G [00:15<00:27, 53.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 782M/2.12G [00:15<00:25, 56.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▋      | 788M/2.12G [00:15<00:28, 50.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 793M/2.12G [00:15<00:27, 52.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 800M/2.12G [00:15<00:26, 54.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 808M/2.12G [00:15<00:27, 52.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 816M/2.12G [00:15<00:29, 47.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 824M/2.12G [00:16<00:25, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 830M/2.12G [00:16<00:28, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 835M/2.12G [00:16<00:27, 50.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 841M/2.12G [00:16<00:25, 53.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 848M/2.12G [00:16<00:26, 52.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 855M/2.12G [00:16<00:25, 53.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 860M/2.12G [00:16<00:26, 50.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 865M/2.12G [00:16<00:32, 41.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 872M/2.12G [00:17<00:29, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 878M/2.12G [00:17<00:29, 46.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 883M/2.12G [00:17<00:31, 42.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 887M/2.12G [00:17<00:34, 39.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 891M/2.12G [00:17<00:34, 38.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████▏     | 896M/2.12G [00:17<00:37, 35.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 904M/2.12G [00:17<00:31, 42.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 912M/2.12G [00:18<00:27, 48.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 919M/2.12G [00:18<00:24, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 925M/2.12G [00:18<00:23, 56.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 931M/2.12G [00:18<00:22, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 936M/2.12G [00:18<00:26, 49.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 943M/2.12G [00:18<00:24, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 948M/2.12G [00:18<00:30, 41.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 955M/2.12G [00:18<00:25, 49.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 961M/2.12G [00:19<00:26, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 966M/2.12G [00:19<00:25, 50.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 972M/2.12G [00:19<00:28, 44.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 976M/2.12G [00:19<00:32, 39.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 984M/2.12G [00:19<00:26, 46.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 992M/2.12G [00:19<00:24, 49.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 0.98G/2.12G [00:19<00:21, 57.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 0.98G/2.12G [00:20<00:23, 50.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 0.99G/2.12G [00:20<00:23, 52.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 0.99G/2.12G [00:20<00:26, 46.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 1.00G/2.12G [00:20<00:23, 51.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.01G/2.12G [00:20<00:22, 53.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.01G/2.12G [00:20<00:22, 53.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.02G/2.12G [00:20<00:26, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.03G/2.12G [00:20<00:23, 50.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▊     | 1.03G/2.12G [00:21<00:22, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.04G/2.12G [00:21<00:21, 54.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.05G/2.12G [00:21<00:19, 57.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 1.05G/2.12G [00:21<00:19, 58.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 1.06G/2.12G [00:21<00:17, 65.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.07G/2.12G [00:21<00:18, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.08G/2.12G [00:21<00:17, 64.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.08G/2.12G [00:21<00:19, 58.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████▏    | 1.09G/2.12G [00:22<00:18, 58.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.09G/2.12G [00:22<00:20, 54.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.10G/2.12G [00:22<00:17, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.11G/2.12G [00:22<00:18, 59.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.11G/2.12G [00:22<00:19, 54.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.12G/2.12G [00:22<00:21, 50.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.12G/2.12G [00:22<00:22, 48.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.13G/2.12G [00:23<00:21, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▎    | 1.14G/2.12G [00:23<00:21, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.14G/2.12G [00:23<00:22, 45.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.15G/2.12G [00:23<00:22, 46.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 1.16G/2.12G [00:23<00:19, 54.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 1.16G/2.12G [00:23<00:20, 50.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:23<00:22, 44.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:23<00:22, 45.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:24<00:30, 33.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.18G/2.12G [00:24<00:35, 28.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.18G/2.12G [00:24<00:34, 29.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.19G/2.12G [00:24<00:33, 29.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.19G/2.12G [00:24<00:34, 28.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▋    | 1.20G/2.12G [00:24<00:28, 35.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.20G/2.12G [00:24<00:22, 43.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.21G/2.12G [00:25<00:19, 50.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.22G/2.12G [00:25<00:17, 54.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.23G/2.12G [00:25<00:17, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.23G/2.12G [00:25<00:18, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▊    | 1.24G/2.12G [00:25<00:15, 59.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.25G/2.12G [00:25<00:15, 60.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.26G/2.12G [00:25<00:14, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 1.26G/2.12G [00:26<00:14, 62.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 1.27G/2.12G [00:26<00:15, 57.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 1.28G/2.12G [00:26<00:15, 56.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 1.28G/2.12G [00:26<00:15, 57.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.29G/2.12G [00:26<00:15, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.30G/2.12G [00:26<00:14, 59.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████▏   | 1.30G/2.12G [00:26<00:16, 52.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.31G/2.12G [00:26<00:17, 50.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.31G/2.12G [00:26<00:13, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.32G/2.12G [00:27<00:17, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.33G/2.12G [00:27<00:16, 50.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.33G/2.12G [00:27<00:16, 50.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.34G/2.12G [00:27<00:17, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▎   | 1.35G/2.12G [00:27<00:14, 57.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.35G/2.12G [00:27<00:13, 59.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.36G/2.12G [00:27<00:13, 59.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.36G/2.12G [00:28<00:15, 53.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 1.37G/2.12G [00:28<00:17, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 1.37G/2.12G [00:28<00:16, 48.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 1.38G/2.12G [00:28<00:14, 56.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.39G/2.12G [00:28<00:12, 61.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.40G/2.12G [00:28<00:13, 57.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.40G/2.12G [00:28<00:13, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 1.41G/2.12G [00:28<00:14, 52.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.41G/2.12G [00:29<00:18, 40.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.42G/2.12G [00:29<00:18, 41.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.42G/2.12G [00:29<00:16, 45.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.43G/2.12G [00:29<00:15, 46.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.43G/2.12G [00:29<00:15, 47.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.44G/2.12G [00:29<00:14, 49.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.44G/2.12G [00:29<00:15, 46.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.45G/2.12G [00:29<00:18, 39.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▊   | 1.45G/2.12G [00:30<00:15, 47.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.46G/2.12G [00:30<00:15, 46.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.47G/2.12G [00:30<00:14, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.47G/2.12G [00:30<00:15, 46.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 1.48G/2.12G [00:30<00:15, 45.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 1.49G/2.12G [00:30<00:11, 57.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 1.49G/2.12G [00:30<00:11, 60.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.50G/2.12G [00:30<00:11, 59.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.51G/2.12G [00:31<00:10, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████▏  | 1.51G/2.12G [00:31<00:10, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.52G/2.12G [00:31<00:10, 60.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.52G/2.12G [00:31<00:13, 47.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.53G/2.12G [00:31<00:14, 45.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.53G/2.12G [00:31<00:13, 47.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.54G/2.12G [00:31<00:16, 38.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.55G/2.12G [00:31<00:12, 48.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.55G/2.12G [00:32<00:12, 49.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 1.56G/2.12G [00:32<00:13, 45.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 1.56G/2.12G [00:32<00:13, 43.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.57G/2.12G [00:32<00:14, 42.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.57G/2.12G [00:32<00:15, 38.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.58G/2.12G [00:32<00:12, 47.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 1.59G/2.12G [00:32<00:10, 52.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 1.59G/2.12G [00:32<00:09, 59.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.60G/2.12G [00:33<00:09, 61.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.61G/2.12G [00:33<00:09, 57.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▋  | 1.62G/2.12G [00:33<00:09, 57.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.62G/2.12G [00:33<00:09, 54.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.63G/2.12G [00:33<00:09, 55.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.64G/2.12G [00:33<00:09, 55.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.64G/2.12G [00:34<00:10, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.65G/2.12G [00:34<00:10, 50.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.66G/2.12G [00:34<00:10, 47.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 1.66G/2.12G [00:34<00:09, 51.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.67G/2.12G [00:34<00:08, 57.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.68G/2.12G [00:34<00:07, 59.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.68G/2.12G [00:34<00:08, 58.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 1.69G/2.12G [00:35<00:11, 40.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 1.70G/2.12G [00:35<00:09, 48.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 1.70G/2.12G [00:35<00:09, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.71G/2.12G [00:35<00:08, 51.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.71G/2.12G [00:35<00:08, 48.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.72G/2.12G [00:35<00:09, 46.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.73G/2.12G [00:35<00:07, 57.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.73G/2.12G [00:35<00:07, 56.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.74G/2.12G [00:36<00:08, 50.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.74G/2.12G [00:36<00:08, 49.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.75G/2.12G [00:36<00:06, 58.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.76G/2.12G [00:36<00:06, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.77G/2.12G [00:36<00:06, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▎ | 1.77G/2.12G [00:36<00:06, 59.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.78G/2.12G [00:36<00:05, 70.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.79G/2.12G [00:36<00:05, 64.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 1.80G/2.12G [00:36<00:05, 65.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.80G/2.12G [00:37<00:05, 57.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.81G/2.12G [00:37<00:05, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.82G/2.12G [00:37<00:07, 42.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.82G/2.12G [00:37<00:07, 45.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▋ | 1.83G/2.12G [00:37<00:06, 46.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.84G/2.12G [00:37<00:05, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.84G/2.12G [00:37<00:05, 51.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.85G/2.12G [00:38<00:06, 46.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.85G/2.12G [00:38<00:06, 46.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.86G/2.12G [00:38<00:05, 49.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.87G/2.12G [00:38<00:05, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.87G/2.12G [00:38<00:04, 54.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.88G/2.12G [00:38<00:04, 58.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.89G/2.12G [00:38<00:04, 57.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.90G/2.12G [00:39<00:04, 55.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 1.90G/2.12G [00:39<00:04, 47.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 1.91G/2.12G [00:39<00:05, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 1.91G/2.12G [00:39<00:05, 43.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 1.92G/2.12G [00:39<00:04, 46.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.92G/2.12G [00:39<00:04, 52.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.93G/2.12G [00:39<00:04, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.93G/2.12G [00:40<00:04, 40.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████▏| 1.94G/2.12G [00:40<00:04, 43.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:40<00:03, 52.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:40<00:03, 46.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:40<00:04, 42.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.96G/2.12G [00:40<00:04, 37.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.96G/2.12G [00:40<00:04, 40.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.97G/2.12G [00:41<00:04, 32.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.97G/2.12G [00:41<00:04, 33.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.98G/2.12G [00:41<00:04, 35.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.98G/2.12G [00:41<00:04, 33.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▎| 1.98G/2.12G [00:41<00:03, 38.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 1.99G/2.12G [00:41<00:02, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 2.00G/2.12G [00:41<00:02, 44.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 2.00G/2.12G [00:41<00:02, 49.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 2.01G/2.12G [00:41<00:02, 46.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 2.02G/2.12G [00:42<00:02, 46.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.02G/2.12G [00:42<00:01, 56.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.03G/2.12G [00:42<00:01, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.04G/2.12G [00:42<00:01, 60.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▋| 2.04G/2.12G [00:42<00:01, 58.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.05G/2.12G [00:42<00:01, 47.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.06G/2.12G [00:42<00:01, 47.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.06G/2.12G [00:43<00:01, 50.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.07G/2.12G [00:43<00:01, 50.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.07G/2.12G [00:43<00:01, 44.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.08G/2.12G [00:43<00:00, 48.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.09G/2.12G [00:43<00:00, 51.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▊| 2.09G/2.12G [00:43<00:00, 46.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.10G/2.12G [00:43<00:00, 40.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.10G/2.12G [00:44<00:00, 44.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.11G/2.12G [00:44<00:00, 38.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 2.11G/2.12G [00:44<00:00, 37.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 2.12G/2.12G [00:44<00:00, 51.2MB/s]\u001b[0m\n",
      "\u001b[34m2022-06-05 12:14:00,286 - __main__ - INFO - STARTING TRAININGGGGGG\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 88.0/88.0 [00:00<00:00, 84.9kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.82M/1.82M [00:00<00:00, 65.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 65.0/65.0 [00:00<00:00, 93.4kB/s]\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 35\u001b[0m\n",
      "\u001b[34mNum examples = 35\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 20\u001b[0m\n",
      "\u001b[34mNum Epochs = 20\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 32\n",
      "  Total optimization steps = 20\u001b[0m\n",
      "\u001b[34m0%|          | 0/20 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.552 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.610 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.610 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.610 algo-1:32 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.611 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.611 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.shared.weight count_params:98409472\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.733 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.734 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.735 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.736 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.737 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.738 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.encoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:591] name:model.decoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:593] Total Trainable Params: 568699904\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.749 algo-1:32 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-06-05 12:14:01.750 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m5%|▌         | 1/20 [00:06<02:12,  6.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.7895, 'learning_rate': 0.0, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m5%|▌         | 1/20 [00:07<02:12,  6.99s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\n",
      "  Batch size = 1\u001b[0m\n",
      "\u001b[34mNum examples = 18\n",
      "  Batch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 3/18 [00:00<00:00, 23.23it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 6/18 [00:00<00:00, 17.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 8/18 [00:00<00:00, 16.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 10/18 [00:00<00:00, 16.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 12/18 [00:00<00:00, 16.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 14/18 [00:00<00:00, 15.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 16/18 [00:00<00:00, 15.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 15.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 12.064590454101562, 'eval_runtime': 1.1703, 'eval_samples_per_second': 15.381, 'eval_steps_per_second': 15.381, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m5%|▌         | 1/20 [00:08<02:12,  6.99s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 15.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-1\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-1\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-1/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m10%|█         | 2/20 [00:17<02:45,  9.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.7807, 'learning_rate': 1e-06, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m10%|█         | 2/20 [00:18<02:45,  9.18s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\n",
      "  Batch size = 1\u001b[0m\n",
      "\u001b[34mNum examples = 18\n",
      "  Batch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 3/18 [00:00<00:00, 22.99it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 6/18 [00:00<00:00, 17.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 8/18 [00:00<00:00, 16.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 10/18 [00:00<00:00, 16.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 12/18 [00:00<00:00, 15.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 14/18 [00:00<00:00, 15.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 16/18 [00:00<00:00, 15.60it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 15.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 12.064590454101562, 'eval_runtime': 1.1785, 'eval_samples_per_second': 15.274, 'eval_steps_per_second': 15.274, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m10%|█         | 2/20 [00:19<02:45,  9.18s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 15.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-2\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-2\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-2/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m15%|█▌        | 3/20 [01:29<10:41, 37.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 11.4204, 'learning_rate': 1e-06, 'epoch': 2.91}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 3/20 [01:29<10:41, 37.71s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\n",
      "  Batch size = 1\u001b[0m\n",
      "\u001b[34mNum examples = 18\n",
      "  Batch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 3/18 [00:00<00:00, 22.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 6/18 [00:00<00:00, 17.74it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 8/18 [00:00<00:00, 16.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 10/18 [00:00<00:00, 16.25it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 12/18 [00:00<00:00, 15.91it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 14/18 [00:00<00:00, 15.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 16/18 [00:00<00:00, 15.55it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 15.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 12.064590454101562, 'eval_runtime': 1.1814, 'eval_samples_per_second': 15.236, 'eval_steps_per_second': 15.236, 'epoch': 2.91}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 3/20 [01:31<10:41, 37.71s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 15.45it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-3\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-3\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-3/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"neuroscience_to_dev_bio.py\", line 326, in <module>\u001b[0m\n",
      "\u001b[34mtrainer.train()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train\u001b[0m\n",
      "\u001b[34mself._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate\u001b[0m\n",
      "\u001b[34mself._save_checkpoint(model, trial, metrics=metrics)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1557, in _save_checkpoint\u001b[0m\n",
      "\u001b[34mself.save_model(output_dir)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1961, in save_model\u001b[0m\n",
      "\u001b[34mself._save(output_dir)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 2011, in _save\u001b[0m\n",
      "\u001b[34mself.tokenizer.save_pretrained(output_dir)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2044, in save_pretrained\u001b[0m\n",
      "\u001b[34msave_files = self._save_pretrained(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py\", line 579, in _save_pretrained\u001b[0m\n",
      "\u001b[34mself.backend_tokenizer.save(tokenizer_file)\u001b[0m\n",
      "\u001b[34mException: No space left on device (os error 28)\u001b[0m\n",
      "\u001b[34m15%|█▌        | 3/20 [01:52<10:40, 37.67s/it]\u001b[0m\n",
      "\u001b[34m2022-06-05 12:15:55,118 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-06-05 12:15:55,118 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience-to-dev-bio\"\u001b[0m\n",
      "\u001b[34m2022-06-05 12:15:55,118 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-06-05 12:16:09 Uploading - Uploading generated training model\n",
      "2022-06-05 12:16:09 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2022-06-05-12-06-38-531: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience-to-dev-bio\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d691e455ab0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1992\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3824\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3825\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3826\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3363\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3366\u001b[0m             )\n\u001b[1;32m   3367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2022-06-05-12-06-38-531: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"\"\nCommand \"/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience-to-dev-bio\", exit code: 1"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"Testing\"}\n",
    "\n",
    "print('Predicting...')\n",
    "print(predictor.predict(sentiment_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
