{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"sagemaker>=2.48.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch\n",
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "# print(f\"sagemaker role arn: {role}\")\n",
    "# print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1, 'model_name':'neuroscience_to_dev_bio'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='neuroscience_to_dev_bio.py',\n",
    "                                    source_dir='./scripts',\n",
    "                                    instance_type='ml.g4dn.16xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.12',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    volume_size=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 17:24:35 Starting - Starting the training job...\n",
      "2022-06-05 17:24:59 Starting - Preparing the instances for trainingProfilerReport-1654449875: InProgress\n",
      ".........\n",
      "2022-06-05 17:26:19 Downloading - Downloading input data\n",
      "2022-06-05 17:26:19 Training - Downloading the training image..........................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-05 17:30:41,365 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-05 17:30:41,384 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-05 17:30:41,390 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-05 17:30:41,847 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.1.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (2021.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 3)) (1.19.1)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nltk, absl-py, rouge-score\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.1.0 nltk-3.7 rouge-score-0.0.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-06-05 17:30:45,078 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"neuroscience_to_dev_bio\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-06-05-17-24-35-310\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-17-24-35-310/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"neuroscience_to_dev_bio\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"neuroscience_to_dev_bio.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"neuroscience_to_dev_bio\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=neuroscience_to_dev_bio.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=neuroscience_to_dev_bio\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-17-24-35-310/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"neuroscience_to_dev_bio\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-06-05-17-24-35-310\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-17-24-35-310/source/sourcedir.tar.gz\",\"module_name\":\"neuroscience_to_dev_bio\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"neuroscience_to_dev_bio.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"neuroscience_to_dev_bio\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=neuroscience_to_dev_bio\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience_to_dev_bio\u001b[0m\n",
      "\u001b[34m**** TRAINING!\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 5.61kB [00:00, 3.45MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/3.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 3.02k/3.02k [00:00<00:00, 2.87MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 9.54M/2.12G [00:00<00:22, 100MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 19.4M/2.12G [00:00<00:22, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|▏         | 29.4M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 39.2M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 49.1M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 58.9M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 68.8M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▎         | 78.7M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 88.6M/2.12G [00:00<00:21, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 98.4M/2.12G [00:01<00:20, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 108M/2.12G [00:01<00:20, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 118M/2.12G [00:01<00:20, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 128M/2.12G [00:01<00:20, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 138M/2.12G [00:01<00:20, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 148M/2.12G [00:01<00:20, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 158M/2.12G [00:01<00:20, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 168M/2.12G [00:01<00:20, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 178M/2.12G [00:01<00:20, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▊         | 188M/2.12G [00:01<00:20, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 197M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 207M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 217M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 227M/2.12G [00:02<00:19, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 237M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█▏        | 247M/2.12G [00:02<00:19, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 257M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 267M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 276M/2.12G [00:02<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 286M/2.12G [00:02<00:19, 104MB/s]\u001b[0m\n",
      "\n",
      "2022-06-05 17:31:00 Training - Training image download completed. Training in progress.\u001b[34mDownloading:  14%|█▎        | 296M/2.12G [00:03<00:19, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 306M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 316M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 326M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 336M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 346M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 356M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 366M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 376M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 385M/2.12G [00:03<00:18, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 396M/2.12G [00:04<00:17, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 407M/2.12G [00:04<00:16, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 419M/2.12G [00:04<00:16, 112MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 430M/2.12G [00:04<00:15, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 441M/2.12G [00:04<00:15, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 453M/2.12G [00:04<00:15, 118MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██▏       | 465M/2.12G [00:04<00:15, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 476M/2.12G [00:04<00:14, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 488M/2.12G [00:04<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 499M/2.12G [00:04<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 510M/2.12G [00:05<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 522M/2.12G [00:05<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 533M/2.12G [00:05<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 545M/2.12G [00:05<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 556M/2.12G [00:05<00:14, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 568M/2.12G [00:05<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 579M/2.12G [00:05<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 591M/2.12G [00:05<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 602M/2.12G [00:05<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 614M/2.12G [00:05<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 625M/2.12G [00:06<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 637M/2.12G [00:06<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 648M/2.12G [00:06<00:13, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 660M/2.12G [00:06<00:13, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 672M/2.12G [00:06<00:13, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███▏      | 683M/2.12G [00:06<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 695M/2.12G [00:06<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 706M/2.12G [00:06<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 718M/2.12G [00:06<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 729M/2.12G [00:06<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 741M/2.12G [00:07<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 752M/2.12G [00:07<00:12, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 764M/2.12G [00:07<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 775M/2.12G [00:07<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▋      | 787M/2.12G [00:07<00:12, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 798M/2.12G [00:07<00:11, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 810M/2.12G [00:07<00:11, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 821M/2.12G [00:07<00:11, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 833M/2.12G [00:07<00:11, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 844M/2.12G [00:07<00:11, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 856M/2.12G [00:08<00:11, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 867M/2.12G [00:08<00:11, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 879M/2.12G [00:08<00:11, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 890M/2.12G [00:08<00:11, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 902M/2.12G [00:08<00:11, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 914M/2.12G [00:08<00:11, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 925M/2.12G [00:08<00:11, 117MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 936M/2.12G [00:08<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 948M/2.12G [00:08<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 959M/2.12G [00:08<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 971M/2.12G [00:09<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 982M/2.12G [00:09<00:10, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 994M/2.12G [00:09<00:10, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 0.98G/2.12G [00:09<00:10, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 0.99G/2.12G [00:09<00:10, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 1.00G/2.12G [00:09<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.02G/2.12G [00:09<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.03G/2.12G [00:09<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.04G/2.12G [00:09<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 1.05G/2.12G [00:09<00:09, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 1.06G/2.12G [00:10<00:10, 111MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.07G/2.12G [00:10<00:10, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.08G/2.12G [00:10<00:10, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.09G/2.12G [00:10<00:10, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.10G/2.12G [00:10<00:10, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.11G/2.12G [00:10<00:10, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.12G/2.12G [00:10<00:10, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.13G/2.12G [00:10<00:10, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.14G/2.12G [00:10<00:10, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.15G/2.12G [00:10<00:09, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 1.16G/2.12G [00:11<00:09, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:11<00:09, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.18G/2.12G [00:11<00:09, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.19G/2.12G [00:11<00:09, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.20G/2.12G [00:11<00:09, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.21G/2.12G [00:11<00:09, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.22G/2.12G [00:11<00:09, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.23G/2.12G [00:11<00:09, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.24G/2.12G [00:11<00:09, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.25G/2.12G [00:11<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.26G/2.12G [00:12<00:08, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 1.27G/2.12G [00:12<00:08, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 1.28G/2.12G [00:12<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.29G/2.12G [00:12<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.30G/2.12G [00:12<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.31G/2.12G [00:12<00:08, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.32G/2.12G [00:12<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.33G/2.12G [00:12<00:08, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.34G/2.12G [00:12<00:08, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▎   | 1.35G/2.12G [00:12<00:07, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.36G/2.12G [00:13<00:07, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.37G/2.12G [00:13<00:07, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 1.38G/2.12G [00:13<00:07, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 1.38G/2.12G [00:13<00:07, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.39G/2.12G [00:13<00:07, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 1.40G/2.12G [00:13<00:07, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.41G/2.12G [00:13<00:07, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.42G/2.12G [00:13<00:07, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.43G/2.12G [00:13<00:07, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.44G/2.12G [00:14<00:12, 58.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▊   | 1.45G/2.12G [00:14<00:10, 66.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.46G/2.12G [00:14<00:09, 74.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.47G/2.12G [00:14<00:08, 81.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 1.48G/2.12G [00:14<00:07, 87.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 1.49G/2.12G [00:14<00:07, 92.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.50G/2.12G [00:14<00:06, 95.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████▏  | 1.51G/2.12G [00:14<00:06, 98.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.52G/2.12G [00:15<00:06, 100MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.53G/2.12G [00:15<00:06, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.54G/2.12G [00:15<00:06, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.55G/2.12G [00:15<00:06, 99.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 1.56G/2.12G [00:15<00:05, 101MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.57G/2.12G [00:15<00:05, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 1.58G/2.12G [00:15<00:05, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 1.59G/2.12G [00:15<00:05, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 1.60G/2.12G [00:15<00:05, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.61G/2.12G [00:15<00:05, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▋  | 1.62G/2.12G [00:16<00:05, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.63G/2.12G [00:16<00:05, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.64G/2.12G [00:16<00:05, 103MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.65G/2.12G [00:16<00:04, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.66G/2.12G [00:16<00:04, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▊  | 1.67G/2.12G [00:16<00:04, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.68G/2.12G [00:16<00:04, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 1.69G/2.12G [00:16<00:04, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 1.70G/2.12G [00:16<00:04, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.71G/2.12G [00:16<00:04, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.72G/2.12G [00:17<00:04, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████▏ | 1.73G/2.12G [00:17<00:03, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.74G/2.12G [00:17<00:03, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.75G/2.12G [00:17<00:03, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.76G/2.12G [00:17<00:03, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.77G/2.12G [00:17<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.78G/2.12G [00:17<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.79G/2.12G [00:17<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 1.80G/2.12G [00:17<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.81G/2.12G [00:17<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.82G/2.12G [00:18<00:03, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.83G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.83G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.84G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.85G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.86G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.87G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.88G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.89G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 1.90G/2.12G [00:18<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 1.91G/2.12G [00:19<00:02, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.92G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.93G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.94G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.96G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.97G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▎| 1.98G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 1.99G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 2.00G/2.12G [00:19<00:01, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 2.01G/2.12G [00:20<00:01, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 2.02G/2.12G [00:20<00:00, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.03G/2.12G [00:20<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▋| 2.04G/2.12G [00:20<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.05G/2.12G [00:20<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.06G/2.12G [00:20<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.07G/2.12G [00:20<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.08G/2.12G [00:20<00:00, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▊| 2.09G/2.12G [00:20<00:00, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.10G/2.12G [00:20<00:00, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 2.11G/2.12G [00:21<00:00, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 2.12G/2.12G [00:21<00:00, 107MB/s]\u001b[0m\n",
      "\u001b[34m2022-06-05 17:31:30,055 - __main__ - INFO - STARTING TRAINING\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 88.0/88.0 [00:00<00:00, 127kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.82M/1.82M [00:00<00:00, 67.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 65.0/65.0 [00:00<00:00, 77.6kB/s]\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 35\u001b[0m\n",
      "\u001b[34mNum Epochs = 20\n",
      "  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 35\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 80\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 80\u001b[0m\n",
      "\u001b[34m0%|          | 0/80 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.571 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.650 algo-1:32 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.651 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.651 algo-1:32 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.652 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.652 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.shared.weight count_params:98409472\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.784 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.785 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.786 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.787 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.788 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.789 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.790 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.encoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:591] name:model.decoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:593] Total Trainable Params: 568699904\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.803 algo-1:32 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-06-05 17:31:31.804 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m1%|▏         | 1/80 [00:03<04:47,  3.64s/it]\u001b[0m\n",
      "\u001b[34m2%|▎         | 2/80 [00:09<06:13,  4.79s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/80 [00:11<04:34,  3.57s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/80 [00:13<03:39,  2.88s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.7721, 'learning_rate': 1e-05, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/80 [00:13<03:39,  2.88s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 3/18 [00:00<00:00, 15.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 5/18 [00:00<00:01, 12.31it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 7/18 [00:00<00:00, 11.30it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 9/18 [00:00<00:00, 10.89it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 11/18 [00:00<00:00, 10.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 13/18 [00:01<00:00, 10.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 15/18 [00:01<00:00, 10.43it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 17/18 [00:01<00:00, 10.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 11.975937843322754, 'eval_runtime': 1.7671, 'eval_samples_per_second': 10.186, 'eval_steps_per_second': 10.186, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/80 [00:15<03:39,  2.88s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 10.32it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-4\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-4\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m6%|▋         | 5/80 [00:59<23:10, 18.54s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/80 [01:01<15:54, 12.89s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/80 [01:03<11:20,  9.32s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 8/80 [01:05<08:22,  6.98s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.6091, 'learning_rate': 2.5e-05, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m10%|█         | 8/80 [01:05<08:22,  6.98s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 3/18 [00:00<00:00, 15.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 5/18 [00:00<00:01, 12.77it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 7/18 [00:00<00:00, 11.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 9/18 [00:00<00:00, 11.33it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 11/18 [00:00<00:00, 11.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 13/18 [00:01<00:00, 10.96it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 15/18 [00:01<00:00, 10.85it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 17/18 [00:01<00:00, 10.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 11.256613731384277, 'eval_runtime': 1.697, 'eval_samples_per_second': 10.607, 'eval_steps_per_second': 10.607, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m10%|█         | 8/80 [01:07<08:22,  6.98s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:01<00:00, 10.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-8\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-8\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 379, in save\u001b[0m\n",
      "\u001b[34m_save(obj, opened_zipfile, pickle_module, pickle_protocol)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 499, in _save\u001b[0m\n",
      "\u001b[34mzip_file.write_record(name, storage.data_ptr(), num_bytes)\u001b[0m\n",
      "\u001b[34mOSError:\u001b[0m\n",
      "\u001b[34m[Errno 28] No space left on device\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"neuroscience_to_dev_bio.py\", line 327, in <module>\u001b[0m\n",
      "\u001b[34mtrainer.train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train\u001b[0m\n",
      "\u001b[34mself._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate\u001b[0m\n",
      "\u001b[34mself._save_checkpoint(model, trial, metrics=metrics)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint\u001b[0m\n",
      "\u001b[34mtorch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save\u001b[0m\n",
      "\u001b[34mreturn\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__\u001b[0m\n",
      "\u001b[34mself.file_like.write_end_of_file()\u001b[0m\n",
      "\u001b[34mRuntimeError:\u001b[0m\n",
      "\u001b[34m[enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320\u001b[0m\n",
      "\u001b[34mterminate called after throwing an instance of 'c10::Error'\u001b[0m\n",
      "\u001b[34mwhat():  [enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320\u001b[0m\n",
      "\u001b[34mframe #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x68 (0x7fd7c64fc898 in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)\u001b[0m\n",
      "\u001b[34mframe #1: <unknown function> + 0x29c70a7 (0x7fd6672040a7 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #2: <unknown function> + 0x29c2221 (0x7fd6671ff221 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*, unsigned long, bool) + 0xa9 (0x7fd6672061e9 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0x197 (0x7fd667206467 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x16d (0x7fd66720667d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #6: <unknown function> + 0xd8fe4d (0x7fd790464e4d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\u001b[0m\n",
      "\u001b[34mframe #7: <unknown function> + 0xa2ea72 (0x7fd790103a72 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\u001b[0m\n",
      "\u001b[34mframe #8: <unknown function> + 0xa2fa33 (0x7fd790104a33 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\u001b[0m\n",
      "\u001b[34mframe #9: <unknown function> + 0xfaef8 (0x561c89481ef8 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #10: <unknown function> + 0xfd538 (0x561c89484538 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #11: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #12: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #13: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #14: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #15: PyDict_SetItemString + 0x401 (0x561c895283d1 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #16: PyImport_Cleanup + 0xa4 (0x561c895f64e4 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #17: Py_FinalizeEx + 0x7a (0x561c895f6a9a in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #18: Py_RunMain + 0x1b8 (0x561c895fb5c8 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #19: Py_BytesMain + 0x39 (0x561c895fb939 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #20: __libc_start_main + 0xf3 (0x7fd7e9eae0b3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\u001b[0m\n",
      "\u001b[34mframe #21: <unknown function> + 0x1e8f39 (0x561c8956ff39 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mAborted\u001b[0m\n",
      "\u001b[34m2022-06-05 17:33:23,579 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-06-05 17:33:23,580 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 134\u001b[0m\n",
      "\u001b[34mErrorMessage \"OSError:\n",
      " [Errno 28] No space left on device  During handling of the above exception, another exception occurred: Traceback (most recent call last):   File \"neuroscience_to_dev_bio.py\", line 327, in <module> trainer.train() File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate self._save_checkpoint(model, trial, metrics=metrics) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))   File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save return File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__ self.file_like.write_end_of_file() RuntimeError: [enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320 terminate called after throwing an instance of 'c10::Error' what():  [enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320 frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x68 (0x7fd7c64fc898 in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so) frame #1: <unknown function> + 0x29c70a7 (0x7fd6672040a7 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #2: <unknown function> + 0x29c2221 (0x7fd6671ff221 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*, unsigned long, bool) + 0xa9 (0x7fd6672061e9 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0x197 (0x7fd667206467 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x16d (0x7fd66720667d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #6: <unknown function> + 0xd8fe4d (0x7fd790464e4d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so) frame #7: <unknown function> + 0xa2ea72 (0x7fd790103a72 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so) frame #8: <unknown function> + 0xa2fa33 (0x7fd790104a33 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so) frame #9: <unknown function> + 0xfaef8 (0x561c89481ef8 in /opt/conda/bin/python3.8) frame #10: <unknown function> + 0xfd538 (0x561c89484538 in /opt/conda/bin/python3.8) frame #11: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8) frame #12: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8) frame #13: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8) frame #14: <unknown function> + 0xfd5d9 (0x561c894845d9 in /opt/conda/bin/python3.8) frame #15: PyDict_SetItemString + 0x401 (0x561c895283d1 in /opt/conda/bin/python3.8) frame #16: PyImport_Cleanup + 0xa4 (0x561c895f64e4 in /opt/conda/bin/python3.8) frame #17: Py_FinalizeEx + 0x7a (0x561c895f6a9a in /opt/conda/bin/python3.8) frame #18: Py_RunMain + 0x1b8 (0x561c895fb5c8 in /opt/conda/bin/python3.8) frame #19: Py_BytesMain + 0x39 (0x561c895fb939 in /opt/conda/bin/python3.8) frame #20: __libc_start_main + 0xf3 (0x7fd7e9eae0b3 in /usr/lib/x86_64-linux-gnu/libc.so.6) frame #21: <unknown function> + 0x1e8f39 (0x561c8956ff39 in /opt/conda/bin/python3.8) Aborted\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience_to_dev_bio\"\u001b[0m\n",
      "\u001b[34m2022-06-05 17:33:23,580 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-06-05 17:33:41 Uploading - Uploading generated training model\n",
      "2022-06-05 17:33:41 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2022-06-05-17-24-35-310: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \"OSError:\n [Errno 28] No space left on device  During handling of the above exception, another exception occurred: Traceback (most recent call last):   File \"neuroscience_to_dev_bio.py\", line 327, in <module> trainer.train() File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate self._save_checkpoint(model, trial, metrics=metrics) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))   File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save return File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__ self.file_like.write_end",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d691e455ab0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1992\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3824\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3825\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3826\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3363\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3366\u001b[0m             )\n\u001b[1;32m   3367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2022-06-05-17-24-35-310: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \"OSError:\n [Errno 28] No space left on device  During handling of the above exception, another exception occurred: Traceback (most recent call last):   File \"neuroscience_to_dev_bio.py\", line 327, in <module> trainer.train() File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate self._save_checkpoint(model, trial, metrics=metrics) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))   File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save return File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__ self.file_like.write_end"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"Testing\"}\n",
    "\n",
    "print('Predicting...')\n",
    "print(predictor.predict(sentiment_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
