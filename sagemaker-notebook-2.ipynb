{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \"sagemaker>=2.48.0\" \"transformers==4.12.3\" \"datasets[s3]==1.18.3\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.6m/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers torch\n",
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::641078039118:role/service-role/AmazonSageMaker-ExecutionRole-20220603T174026\n",
      "sagemaker bucket: sagemaker-us-east-1-641078039118\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "# print(f\"sagemaker role arn: {role}\")\n",
    "# print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1, 'model_name':'neuroscience_to_dev_bio'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='neuroscience_to_dev_bio.py',\n",
    "                                    source_dir='./scripts',\n",
    "                                    instance_type='ml.g4dn.16xlarge',\n",
    "                                    instance_count=1,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.12',\n",
    "                                    pytorch_version='1.9',\n",
    "                                    py_version='py38',\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    volume_size=900,\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-05 15:47:14 Starting - Starting the training job...ProfilerReport-1654444034: InProgress\n",
      "......\n",
      "2022-06-05 15:48:33 Starting - Preparing the instances for training......\n",
      "2022-06-05 15:49:41 Downloading - Downloading input data\n",
      "2022-06-05 15:49:41 Training - Downloading the training image...........................\n",
      "2022-06-05 15:54:14 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-06-05 15:54:00,394 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-06-05 15:54:00,412 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-06-05 15:54:00,417 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-06-05 15:54:00,746 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.7-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.1.0-py3-none-any.whl (123 kB)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (4.62.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (2021.11.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 1)) (8.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 3)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 3)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: nltk, absl-py, rouge-score\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.1.0 nltk-3.7 rouge-score-0.0.4\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2022-06-05 15:54:03,705 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"model_name\": \"neuroscience-to-dev-bio\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-06-05-15-47-14-048\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-15-47-14-048/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"neuroscience_to_dev_bio\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.16xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.16xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"neuroscience_to_dev_bio.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"neuroscience-to-dev-bio\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=neuroscience_to_dev_bio.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=neuroscience_to_dev_bio\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-15-47-14-048/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"neuroscience-to-dev-bio\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-06-05-15-47-14-048\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-641078039118/huggingface-pytorch-training-2022-06-05-15-47-14-048/source/sourcedir.tar.gz\",\"module_name\":\"neuroscience_to_dev_bio\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.16xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.16xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"neuroscience_to_dev_bio.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"neuroscience-to-dev-bio\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=neuroscience-to-dev-bio\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience-to-dev-bio\u001b[0m\n",
      "\u001b[34m**** TRAINING!\u001b[0m\n",
      "\u001b[34m[nltk_data] Downloading package punkt to /root/nltk_data...\u001b[0m\n",
      "\u001b[34m[nltk_data]   Unzipping tokenizers/punkt.zip.\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.17k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 5.61kB [00:00, 3.51MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/3.02k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 3.02k/3.02k [00:00<00:00, 3.47MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/2.12G [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 8.16M/2.12G [00:00<00:26, 85.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|          | 18.2M/2.12G [00:00<00:23, 97.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   1%|▏         | 28.4M/2.12G [00:00<00:22, 102MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 38.7M/2.12G [00:00<00:21, 104MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 48.8M/2.12G [00:00<00:21, 105MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 59.0M/2.12G [00:00<00:20, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 69.2M/2.12G [00:00<00:20, 106MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▎         | 79.5M/2.12G [00:00<00:20, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   4%|▍         | 89.8M/2.12G [00:00<00:20, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▍         | 100M/2.12G [00:01<00:20, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 110M/2.12G [00:01<00:20, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 120M/2.12G [00:01<00:20, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▌         | 131M/2.12G [00:01<00:19, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   6%|▋         | 141M/2.12G [00:01<00:19, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 151M/2.12G [00:01<00:19, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 162M/2.12G [00:01<00:19, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 172M/2.12G [00:01<00:19, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   8%|▊         | 182M/2.12G [00:01<00:19, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 192M/2.12G [00:01<00:19, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 203M/2.12G [00:02<00:19, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|▉         | 213M/2.12G [00:02<00:19, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  10%|█         | 223M/2.12G [00:02<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 234M/2.12G [00:02<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 244M/2.12G [00:02<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 254M/2.12G [00:02<00:18, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  12%|█▏        | 264M/2.12G [00:02<00:18, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 275M/2.12G [00:02<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 285M/2.12G [00:02<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▎        | 295M/2.12G [00:02<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  14%|█▍        | 306M/2.12G [00:03<00:18, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 316M/2.12G [00:03<00:17, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▌        | 326M/2.12G [00:03<00:17, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 337M/2.12G [00:03<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▌        | 347M/2.12G [00:03<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 357M/2.12G [00:03<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 367M/2.12G [00:03<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  17%|█▋        | 377M/2.12G [00:03<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 388M/2.12G [00:03<00:17, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 398M/2.12G [00:03<00:17, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 408M/2.12G [00:04<00:17, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  19%|█▉        | 419M/2.12G [00:04<00:17, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|█▉        | 429M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 439M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 450M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  21%|██        | 460M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 470M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 480M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 491M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  23%|██▎       | 501M/2.12G [00:04<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 511M/2.12G [00:05<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▍       | 522M/2.12G [00:05<00:16, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▍       | 533M/2.12G [00:05<00:15, 110MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  25%|██▌       | 543M/2.12G [00:05<00:15, 111MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 554M/2.12G [00:05<00:15, 111MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 565M/2.12G [00:05<00:15, 110MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 575M/2.12G [00:05<00:15, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 585M/2.12G [00:05<00:15, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 596M/2.12G [00:05<00:15, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 606M/2.12G [00:05<00:15, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  28%|██▊       | 616M/2.12G [00:06<00:15, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 627M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 637M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|██▉       | 647M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  30%|███       | 658M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 668M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███▏      | 678M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 689M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  32%|███▏      | 699M/2.12G [00:06<00:14, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 709M/2.12G [00:06<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 719M/2.12G [00:07<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▎      | 730M/2.12G [00:07<00:14, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  34%|███▍      | 740M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 750M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▌      | 761M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 771M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▌      | 781M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  36%|███▋      | 791M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 802M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 812M/2.12G [00:07<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 822M/2.12G [00:08<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 833M/2.12G [00:08<00:13, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 843M/2.12G [00:08<00:12, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  39%|███▉      | 853M/2.12G [00:08<00:12, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|███▉      | 863M/2.12G [00:08<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 874M/2.12G [00:08<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 884M/2.12G [00:08<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  41%|████      | 894M/2.12G [00:08<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 904M/2.12G [00:08<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 915M/2.12G [00:08<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 925M/2.12G [00:09<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  43%|████▎     | 935M/2.12G [00:09<00:12, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▎     | 945M/2.12G [00:09<00:11, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 956M/2.12G [00:09<00:11, 107MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 966M/2.12G [00:09<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▍     | 976M/2.12G [00:09<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  45%|████▌     | 987M/2.12G [00:09<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 997M/2.12G [00:09<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▋     | 0.98G/2.12G [00:09<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 0.99G/2.12G [00:09<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  47%|████▋     | 1.00G/2.12G [00:10<00:11, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.01G/2.12G [00:10<00:10, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 1.03G/2.12G [00:10<00:10, 113MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.04G/2.12G [00:10<00:10, 115MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  49%|████▉     | 1.05G/2.12G [00:10<00:09, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 1.06G/2.12G [00:10<00:09, 117MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|█████     | 1.07G/2.12G [00:10<00:09, 118MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  51%|█████     | 1.08G/2.12G [00:10<00:09, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.09G/2.12G [00:10<00:09, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 1.10G/2.12G [00:10<00:09, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.12G/2.12G [00:11<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 1.13G/2.12G [00:11<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▎    | 1.14G/2.12G [00:11<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  54%|█████▍    | 1.15G/2.12G [00:11<00:08, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▍    | 1.16G/2.12G [00:11<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 1.17G/2.12G [00:11<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▌    | 1.18G/2.12G [00:11<00:08, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  56%|█████▋    | 1.19G/2.12G [00:11<00:08, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.21G/2.12G [00:11<00:08, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 1.22G/2.12G [00:11<00:08, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.23G/2.12G [00:12<00:07, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  58%|█████▊    | 1.24G/2.12G [00:12<00:07, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 1.25G/2.12G [00:12<00:07, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|█████▉    | 1.26G/2.12G [00:12<00:08, 112MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  60%|██████    | 1.27G/2.12G [00:12<00:08, 111MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.28G/2.12G [00:12<00:08, 110MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 1.29G/2.12G [00:12<00:08, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████▏   | 1.30G/2.12G [00:12<00:08, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.31G/2.12G [00:12<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  62%|██████▏   | 1.32G/2.12G [00:12<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.33G/2.12G [00:13<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 1.34G/2.12G [00:13<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.35G/2.12G [00:13<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  64%|██████▍   | 1.36G/2.12G [00:13<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 1.37G/2.12G [00:13<00:07, 109MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▌   | 1.38G/2.12G [00:13<00:07, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▌   | 1.39G/2.12G [00:13<00:07, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  66%|██████▋   | 1.40G/2.12G [00:13<00:07, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.41G/2.12G [00:13<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 1.42G/2.12G [00:13<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.43G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  68%|██████▊   | 1.44G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▊   | 1.45G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▉   | 1.46G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|██████▉   | 1.47G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 1.49G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.50G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  71%|███████   | 1.51G/2.12G [00:14<00:06, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.52G/2.12G [00:14<00:05, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.53G/2.12G [00:14<00:05, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 1.54G/2.12G [00:15<00:05, 108MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  73%|███████▎  | 1.55G/2.12G [00:15<00:05, 111MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▎  | 1.56G/2.12G [00:15<00:05, 114MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 1.57G/2.12G [00:15<00:05, 116MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▍  | 1.58G/2.12G [00:15<00:04, 117MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  75%|███████▌  | 1.59G/2.12G [00:15<00:04, 118MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.60G/2.12G [00:15<00:04, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 1.61G/2.12G [00:15<00:04, 119MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.62G/2.12G [00:15<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  77%|███████▋  | 1.64G/2.12G [00:15<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.65G/2.12G [00:16<00:04, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 1.66G/2.12G [00:16<00:04, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.67G/2.12G [00:16<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  79%|███████▉  | 1.68G/2.12G [00:16<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 1.69G/2.12G [00:16<00:03, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|████████  | 1.70G/2.12G [00:16<00:03, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████  | 1.71G/2.12G [00:16<00:03, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  81%|████████▏ | 1.73G/2.12G [00:16<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 1.74G/2.12G [00:16<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.75G/2.12G [00:16<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 1.76G/2.12G [00:17<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▎ | 1.77G/2.12G [00:17<00:03, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  84%|████████▍ | 1.78G/2.12G [00:17<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▍ | 1.79G/2.12G [00:17<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 1.81G/2.12G [00:17<00:02, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.82G/2.12G [00:17<00:02, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  86%|████████▌ | 1.83G/2.12G [00:17<00:02, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.84G/2.12G [00:17<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 1.85G/2.12G [00:17<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.86G/2.12G [00:17<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  88%|████████▊ | 1.87G/2.12G [00:18<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.88G/2.12G [00:18<00:02, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 1.90G/2.12G [00:18<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|████████▉ | 1.91G/2.12G [00:18<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  90%|█████████ | 1.92G/2.12G [00:18<00:01, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████ | 1.93G/2.12G [00:18<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.94G/2.12G [00:18<00:01, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  92%|█████████▏| 1.95G/2.12G [00:18<00:01, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.96G/2.12G [00:18<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 1.97G/2.12G [00:18<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▎| 1.98G/2.12G [00:19<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  94%|█████████▍| 2.00G/2.12G [00:19<00:01, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▍| 2.01G/2.12G [00:19<00:00, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 2.02G/2.12G [00:19<00:00, 121MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▌| 2.03G/2.12G [00:19<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  96%|█████████▋| 2.04G/2.12G [00:19<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.05G/2.12G [00:19<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 2.06G/2.12G [00:19<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.07G/2.12G [00:19<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  98%|█████████▊| 2.09G/2.12G [00:19<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 2.10G/2.12G [00:20<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|█████████▉| 2.11G/2.12G [00:20<00:00, 120MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 2.12G/2.12G [00:20<00:00, 112MB/s]\u001b[0m\n",
      "\u001b[34m2022-06-05 15:54:58,555 - __main__ - INFO - STARTING TRAININGGGGGG\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/88.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 88.0/88.0 [00:00<00:00, 120kB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/1.82M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 1.82M/1.82M [00:00<00:00, 94.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/65.0 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 65.0/65.0 [00:00<00:00, 80.7kB/s]\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34mUsing amp fp16 backend\u001b[0m\n",
      "\u001b[34m***** Running training *****\u001b[0m\n",
      "\u001b[34mNum examples = 35\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m***** Running training *****\n",
      "  Num examples = 35\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 80\u001b[0m\n",
      "\u001b[34mTotal train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 80\u001b[0m\n",
      "\u001b[34m0%|          | 0/80 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:54:59.967 algo-1:32 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.038 algo-1:32 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.040 algo-1:32 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.044 algo-1:32 INFO hook.py:200] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.046 algo-1:32 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.047 algo-1:32 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.424 algo-1:32 INFO hook.py:591] name:model.shared.weight count_params:98409472\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.426 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.426 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.427 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.428 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.428 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.429 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.430 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.430 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.431 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.432 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.433 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.433 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.434 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.435 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.435 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.436 algo-1:32 INFO hook.py:591] name:model.encoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.437 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.438 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.438 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.439 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.440 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.440 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.441 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.442 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.442 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.443 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.444 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.444 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.445 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.446 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.446 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.447 algo-1:32 INFO hook.py:591] name:model.encoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.448 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.449 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.449 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.450 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.451 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.451 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.452 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.453 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.454 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.454 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.455 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.456 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.456 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.457 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.458 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.458 algo-1:32 INFO hook.py:591] name:model.encoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.459 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.460 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.461 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.461 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.462 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.463 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.463 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.464 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.465 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.465 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.466 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.467 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.467 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.468 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.469 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.470 algo-1:32 INFO hook.py:591] name:model.encoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.470 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.471 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.472 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.472 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.473 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.474 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.475 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.475 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.476 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.477 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.477 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.478 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.479 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.479 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.480 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.481 algo-1:32 INFO hook.py:591] name:model.encoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.482 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.482 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.483 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.484 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.484 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.485 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.486 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.486 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.487 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.488 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.488 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.489 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.490 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.490 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.491 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.492 algo-1:32 INFO hook.py:591] name:model.encoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.493 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.493 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.494 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.495 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.495 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.496 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.497 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.497 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.498 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.499 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.500 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.500 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.501 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.502 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.502 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.503 algo-1:32 INFO hook.py:591] name:model.encoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.504 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.505 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.505 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.506 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.507 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.507 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.508 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.509 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.510 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.510 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.511 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.512 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.512 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.513 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.514 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.514 algo-1:32 INFO hook.py:591] name:model.encoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.515 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.516 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.517 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.517 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.518 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.519 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.519 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.520 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.521 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.521 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.522 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.523 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.524 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.524 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.525 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.526 algo-1:32 INFO hook.py:591] name:model.encoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.526 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.527 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.528 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.528 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.529 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.530 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.531 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.531 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.532 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.533 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.533 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.534 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.535 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.535 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.536 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.537 algo-1:32 INFO hook.py:591] name:model.encoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.538 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.538 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.539 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.540 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.540 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.541 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.542 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.542 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.543 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.544 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.544 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.545 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.546 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.546 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.547 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.548 algo-1:32 INFO hook.py:591] name:model.encoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.549 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.549 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.550 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.551 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.552 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.552 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.553 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.554 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.554 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.555 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.556 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.556 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.557 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.558 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.558 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.559 algo-1:32 INFO hook.py:591] name:model.encoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.560 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.561 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.561 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.562 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.563 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.563 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.564 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.565 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.565 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.566 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.567 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.567 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.568 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.569 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.570 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.570 algo-1:32 INFO hook.py:591] name:model.encoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.571 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.572 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.572 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.573 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.574 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.575 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.575 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.576 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.577 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.577 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.578 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.579 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.579 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.580 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.581 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.581 algo-1:32 INFO hook.py:591] name:model.encoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.582 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.583 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.584 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.584 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.585 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.586 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.586 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.587 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.588 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.588 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.589 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.590 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.591 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.591 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.592 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.592 algo-1:32 INFO hook.py:591] name:model.encoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.593 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.594 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.595 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.595 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.596 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.597 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.598 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.598 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.599 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.600 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.600 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.601 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.602 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.602 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.603 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.604 algo-1:32 INFO hook.py:591] name:model.encoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.604 algo-1:32 INFO hook.py:591] name:model.encoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.605 algo-1:32 INFO hook.py:591] name:model.encoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.606 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.607 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.608 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.608 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.609 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.610 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.610 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.611 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.612 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.612 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.613 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.614 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.615 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.615 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.616 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.617 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.617 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.618 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.619 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.619 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.620 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.621 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.622 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.622 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.623 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.624 algo-1:32 INFO hook.py:591] name:model.decoder.layers.0.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.624 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.625 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.626 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.626 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.627 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.628 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.629 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.629 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.630 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.631 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.631 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.632 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.633 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.633 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.634 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.635 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.636 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.636 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.637 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.638 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.638 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.639 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.640 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.640 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.641 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.642 algo-1:32 INFO hook.py:591] name:model.decoder.layers.1.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.643 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.643 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.644 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.645 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.645 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.646 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.647 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.647 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.648 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.649 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.650 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.650 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.651 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.652 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.652 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.653 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.654 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.654 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.655 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.656 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.657 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.657 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.658 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.659 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.659 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.660 algo-1:32 INFO hook.py:591] name:model.decoder.layers.2.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.661 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.661 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.662 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.663 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.664 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.664 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.665 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.666 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.666 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.667 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.668 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.668 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.669 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.670 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.671 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.671 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.672 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.673 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.673 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.674 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.675 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.675 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.676 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.677 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.677 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.678 algo-1:32 INFO hook.py:591] name:model.decoder.layers.3.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.679 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.680 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.680 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.681 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.682 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.682 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.683 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.684 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.684 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.685 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.686 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.687 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.687 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.688 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.689 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.689 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.690 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.691 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.691 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.692 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.693 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.693 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.694 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.695 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.695 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.696 algo-1:32 INFO hook.py:591] name:model.decoder.layers.4.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.697 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.698 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.698 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.699 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.700 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.700 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.701 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.702 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.702 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.703 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.704 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.705 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.705 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.706 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.707 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.707 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.708 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.709 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.709 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.710 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.711 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.711 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.712 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.713 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.714 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.714 algo-1:32 INFO hook.py:591] name:model.decoder.layers.5.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.715 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.716 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.716 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.717 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.718 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.718 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.719 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.720 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.721 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.721 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.722 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.723 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.723 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.724 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.725 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.725 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.726 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.727 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.728 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.728 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.729 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.730 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.730 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.731 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.732 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.732 algo-1:32 INFO hook.py:591] name:model.decoder.layers.6.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.733 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.734 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.735 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.735 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.736 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.737 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.737 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.738 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.739 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.740 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.741 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.742 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.743 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.744 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.745 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.746 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.747 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.748 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.749 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.750 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.750 algo-1:32 INFO hook.py:591] name:model.decoder.layers.7.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.751 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.752 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.753 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.753 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.754 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.755 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.756 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.756 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.757 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.758 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.758 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.759 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.760 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.760 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.761 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.762 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.763 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.763 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.764 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.765 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.765 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.766 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.767 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.767 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.768 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.769 algo-1:32 INFO hook.py:591] name:model.decoder.layers.8.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.770 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.770 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.771 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.772 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.772 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.773 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.774 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.774 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.775 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.776 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.777 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.777 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.778 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.779 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.779 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.780 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.781 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.781 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.782 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.783 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.783 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.784 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.785 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.786 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.786 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.787 algo-1:32 INFO hook.py:591] name:model.decoder.layers.9.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.788 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.788 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.789 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.790 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.791 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.792 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.793 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.794 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.795 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.796 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.797 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.798 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.799 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.800 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.801 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.802 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.803 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.804 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.804 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.805 algo-1:32 INFO hook.py:591] name:model.decoder.layers.10.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.806 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.807 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.807 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.808 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.809 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.809 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.810 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.811 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.811 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.812 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.813 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.814 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.814 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.815 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.816 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.816 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.817 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.818 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.818 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.819 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.820 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.820 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.821 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.822 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.823 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.823 algo-1:32 INFO hook.py:591] name:model.decoder.layers.11.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.824 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.825 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.826 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.826 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.827 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.828 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.828 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.829 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.830 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.830 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.831 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.832 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.832 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.833 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.834 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.835 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.835 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.836 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.837 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.837 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.838 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.839 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.839 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.840 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.841 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.841 algo-1:32 INFO hook.py:591] name:model.decoder.layers.12.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.842 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.843 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.844 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.844 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.845 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.846 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.846 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.847 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.848 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.848 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.849 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.850 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.851 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.851 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.852 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.853 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.853 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.854 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.855 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.855 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.856 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.857 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.858 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.858 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.859 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.860 algo-1:32 INFO hook.py:591] name:model.decoder.layers.13.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.860 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.861 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.862 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.862 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.863 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.864 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.865 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.865 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.866 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.867 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.867 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.868 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.869 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.869 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.870 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.871 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.872 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.872 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.873 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.874 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.874 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.875 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.876 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.876 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.877 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.878 algo-1:32 INFO hook.py:591] name:model.decoder.layers.14.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.879 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.879 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.880 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.881 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.881 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.882 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.883 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.883 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.884 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.885 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.self_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.886 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.k_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.886 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.k_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.887 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.v_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.888 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.v_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.888 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.q_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.889 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.q_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.890 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.out_proj.weight count_params:1048576\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.890 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn.out_proj.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.891 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.892 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.encoder_attn_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.892 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc1.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.893 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc1.bias count_params:4096\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.894 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc2.weight count_params:4194304\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.894 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.fc2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.895 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.final_layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.896 algo-1:32 INFO hook.py:591] name:model.decoder.layers.15.final_layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.897 algo-1:32 INFO hook.py:591] name:model.decoder.layer_norm.weight count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.897 algo-1:32 INFO hook.py:591] name:model.decoder.layer_norm.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.898 algo-1:32 INFO hook.py:593] Total Trainable Params: 568699904\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.899 algo-1:32 INFO hook.py:424] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.901 algo-1:32 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/32-algo-1/prestepzero-*-start-1654444500038414.0_global-0-stepstart-1654444500901361.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:00.905 algo-1:32 INFO hook.py:488] Hook is writing from the hook with pid: 32\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:03.293 algo-1:32 WARNING hook.py:410] The detailed profiling using autograd profiler is not supported for torch version 1.9.1\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m1%|▏         | 1/80 [00:04<05:36,  4.26s/it]\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:04.607 algo-1:32 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/32-algo-1/global-9-stepstart-1654444504334052.5_global-9-forwardpassend-1654444504606463.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-06-05 15:55:05.013 algo-1:32 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/32-algo-1/global-9-forwardpassend-1654444504608278.8_global-10-stepstart-1654444505012674.5/python_stats.\u001b[0m\n",
      "\u001b[34m2%|▎         | 2/80 [00:07<04:24,  3.39s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/80 [00:09<03:41,  2.88s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/80 [00:11<03:13,  2.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.7721, 'learning_rate': 1e-05, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/80 [00:12<03:13,  2.55s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 2/18 [00:00<00:01, 14.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 4/18 [00:00<00:01,  8.97it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 6/18 [00:00<00:01,  7.94it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 7/18 [00:00<00:01,  7.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 8/18 [00:00<00:01,  7.51it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 9/18 [00:01<00:01,  7.35it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 10/18 [00:01<00:01,  7.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 11/18 [00:01<00:00,  7.25it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 12/18 [00:01<00:00,  7.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 13/18 [00:01<00:00,  7.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 14/18 [00:01<00:00,  7.35it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 15/18 [00:01<00:00,  7.21it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 16/18 [00:02<00:00,  7.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 17/18 [00:02<00:00,  7.17it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:02<00:00,  7.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 11.975937843322754, 'eval_runtime': 2.5411, 'eval_samples_per_second': 7.084, 'eval_steps_per_second': 7.084, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/80 [00:14<03:13,  2.55s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:02<00:00,  7.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-4\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-4\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-4/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\u001b[0m\n",
      "\u001b[34m6%|▋         | 5/80 [00:58<23:26, 18.76s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/80 [01:01<16:18, 13.23s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/80 [01:03<11:47,  9.69s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 8/80 [01:06<08:50,  7.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 10.6091, 'learning_rate': 2.5e-05, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m10%|█         | 8/80 [01:06<08:50,  7.37s/it]\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mNum examples = 18\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34mBatch size = 1\u001b[0m\n",
      "\u001b[34m0%|          | 0/18 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 2/18 [00:00<00:01, 12.67it/s]#033[A\u001b[0m\n",
      "\u001b[34m22%|██▏       | 4/18 [00:00<00:01,  7.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 5/18 [00:00<00:01,  7.36it/s]#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 6/18 [00:00<00:01,  6.95it/s]#033[A\u001b[0m\n",
      "\u001b[34m39%|███▉      | 7/18 [00:00<00:01,  6.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 8/18 [00:01<00:01,  6.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m50%|█████     | 9/18 [00:01<00:01,  6.46it/s]#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 10/18 [00:01<00:01,  6.40it/s]#033[A\u001b[0m\n",
      "\u001b[34m61%|██████    | 11/18 [00:01<00:01,  6.69it/s]#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 12/18 [00:01<00:00,  6.79it/s]#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 13/18 [00:01<00:00,  6.98it/s]#033[A\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 14/18 [00:01<00:00,  7.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 15/18 [00:02<00:00,  7.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 16/18 [00:02<00:00,  7.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 17/18 [00:02<00:00,  7.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:02<00:00,  7.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 11.256613731384277, 'eval_runtime': 2.7034, 'eval_samples_per_second': 6.658, 'eval_steps_per_second': 6.658, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m10%|█         | 8/80 [01:09<08:50,  7.37s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 18/18 [00:02<00:00,  7.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-8\u001b[0m\n",
      "\u001b[34mSaving model checkpoint to ./neuroscience-to-dev-bio-translation/checkpoint-8\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/config.json\u001b[0m\n",
      "\u001b[34mConfiguration saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/config.json\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mModel weights saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mtokenizer config file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mSpecial tokens file saved in ./neuroscience-to-dev-bio-translation/checkpoint-8/special_tokens_map.json\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 379, in save\u001b[0m\n",
      "\u001b[34m_save(obj, opened_zipfile, pickle_module, pickle_protocol)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 499, in _save\u001b[0m\n",
      "\u001b[34mzip_file.write_record(name, storage.data_ptr(), num_bytes)\u001b[0m\n",
      "\u001b[34mOSError: [Errno 28] No space left on device\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"neuroscience_to_dev_bio.py\", line 327, in <module>\u001b[0m\n",
      "\u001b[34mtrainer.train()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train\u001b[0m\n",
      "\u001b[34mself._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate\u001b[0m\n",
      "\u001b[34mself._save_checkpoint(model, trial, metrics=metrics)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint\u001b[0m\n",
      "\u001b[34mtorch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save\u001b[0m\n",
      "\u001b[34mreturn\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__\u001b[0m\n",
      "\u001b[34mself.file_like.write_end_of_file()\u001b[0m\n",
      "\u001b[34mRuntimeError:\u001b[0m\n",
      "\u001b[34m[enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320\u001b[0m\n",
      "\u001b[34mterminate called after throwing an instance of 'c10::Error'\u001b[0m\n",
      "\u001b[34mwhat():  [enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320\u001b[0m\n",
      "\u001b[34mframe #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x68 (0x7f5b19ff3898 in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so)\u001b[0m\n",
      "\u001b[34mframe #1: <unknown function> + 0x29c70a7 (0x7f59c34270a7 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #2: <unknown function> + 0x29c2221 (0x7f59c3422221 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*, unsigned long, bool) + 0xa9 (0x7f59c34291e9 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0x197 (0x7f59c3429467 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x16d (0x7f59c342967d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)\u001b[0m\n",
      "\u001b[34mframe #6: <unknown function> + 0xd8fe4d (0x7f59cb90ee4d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\u001b[0m\n",
      "\u001b[34mframe #7: <unknown function> + 0xa2ea72 (0x7f59cb5ada72 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\u001b[0m\n",
      "\u001b[34mframe #8: <unknown function> + 0xa2fa33 (0x7f59cb5aea33 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so)\u001b[0m\n",
      "\u001b[34mframe #9: <unknown function> + 0xfaef8 (0x55e58e223ef8 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #10: <unknown function> + 0xfd538 (0x55e58e226538 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #11: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #12: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #13: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #14: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #15: PyDict_SetItemString + 0x401 (0x55e58e2ca3d1 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #16: PyImport_Cleanup + 0xa4 (0x55e58e3984e4 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #17: Py_FinalizeEx + 0x7a (0x55e58e398a9a in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #18: Py_RunMain + 0x1b8 (0x55e58e39d5c8 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #19: Py_BytesMain + 0x39 (0x55e58e39d939 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mframe #20: __libc_start_main + 0xf3 (0x7f5b3d9a50b3 in /usr/lib/x86_64-linux-gnu/libc.so.6)\u001b[0m\n",
      "\u001b[34mframe #21: <unknown function> + 0x1e8f39 (0x55e58e311f39 in /opt/conda/bin/python3.8)\u001b[0m\n",
      "\u001b[34mAborted\u001b[0m\n",
      "\u001b[34m2022-06-05 15:56:53,887 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-06-05 15:56:53,888 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 134\u001b[0m\n",
      "\u001b[34mErrorMessage \"OSError: [Errno 28] No space left on device\n",
      " During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File \"neuroscience_to_dev_bio.py\", line 327, in <module> trainer.train() File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate self._save_checkpoint(model, trial, metrics=metrics) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME)) File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save return File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__ self.file_like.write_end_of_file() RuntimeError: [enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320 terminate called after throwing an instance of 'c10::Error' what():  [enforce fail at inline_container.cc:298] . unexpected pos 2286570432 vs 2286570320 frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*) + 0x68 (0x7f5b19ff3898 in /opt/conda/lib/python3.8/site-packages/torch/lib/libc10.so) frame #1: <unknown function> + 0x29c70a7 (0x7f59c34270a7 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #2: <unknown function> + 0x29c2221 (0x7f59c3422221 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, void const*, unsigned long, bool) + 0xa9 (0x7f59c34291e9 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0x197 (0x7f59c3429467 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x16d (0x7f59c342967d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so) frame #6: <unknown function> + 0xd8fe4d (0x7f59cb90ee4d in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so) frame #7: <unknown function> + 0xa2ea72 (0x7f59cb5ada72 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so) frame #8: <unknown function> + 0xa2fa33 (0x7f59cb5aea33 in /opt/conda/lib/python3.8/site-packages/torch/lib/libtorch_python.so) frame #9: <unknown function> + 0xfaef8 (0x55e58e223ef8 in /opt/conda/bin/python3.8) frame #10: <unknown function> + 0xfd538 (0x55e58e226538 in /opt/conda/bin/python3.8) frame #11: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8) frame #12: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8) frame #13: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8) frame #14: <unknown function> + 0xfd5d9 (0x55e58e2265d9 in /opt/conda/bin/python3.8) frame #15: PyDict_SetItemString + 0x401 (0x55e58e2ca3d1 in /opt/conda/bin/python3.8) frame #16: PyImport_Cleanup + 0xa4 (0x55e58e3984e4 in /opt/conda/bin/python3.8) frame #17: Py_FinalizeEx + 0x7a (0x55e58e398a9a in /opt/conda/bin/python3.8) frame #18: Py_RunMain + 0x1b8 (0x55e58e39d5c8 in /opt/conda/bin/python3.8) frame #19: Py_BytesMain + 0x39 (0x55e58e39d939 in /opt/conda/bin/python3.8) frame #20: __libc_start_main + 0xf3 (0x7f5b3d9a50b3 in /usr/lib/x86_64-linux-gnu/libc.so.6) frame #21: <unknown function> + 0x1e8f39 (0x55e58e311f39 in /opt/conda/bin/python3.8) Aborted\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 neuroscience_to_dev_bio.py --epochs 1 --model_name neuroscience-to-dev-bio\"\u001b[0m\n",
      "\u001b[34m2022-06-05 15:56:53,888 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-06-05 15:57:15 Uploading - Uploading generated training model\n",
      "2022-06-05 15:57:15 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2022-06-05-15-47-14-048: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \"OSError: [Errno 28] No space left on device\n During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File \"neuroscience_to_dev_bio.py\", line 327, in <module> trainer.train() File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate self._save_checkpoint(model, trial, metrics=metrics) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME)) File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save return File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__ self.file_like.write_end_o",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d691e455ab0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job with our uploaded datasets as input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1992\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3823\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3824\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3825\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3826\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3363\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3366\u001b[0m             )\n\u001b[1;32m   3367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2022-06-05-15-47-14-048: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 134\nErrorMessage \"OSError: [Errno 28] No space left on device\n During handling of the above exception, another exception occurred:  Traceback (most recent call last):   File \"neuroscience_to_dev_bio.py\", line 327, in <module> trainer.train() File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1391, in train self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1495, in _maybe_log_save_evaluate self._save_checkpoint(model, trial, metrics=metrics) File \"/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\", line 1587, in _save_checkpoint torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME)) File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 380, in save return File \"/opt/conda/lib/python3.8/site-packages/torch/serialization.py\", line 259, in __exit__ self.file_like.write_end_o"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_input= {\"inputs\":\"Testing\"}\n",
    "\n",
    "print('Predicting...')\n",
    "print(predictor.predict(sentiment_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the endpoint again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old training job to an estimator \n",
    "\n",
    "In Sagemaker you can attach an old training job to an estimator to continue training, get results etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
